{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyKSkaiz09RP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class TinyBERT:\n",
        "    def __init__(self):\n",
        "        self.word_to_id = {'the': 0, 'cat': 1, 'sits': 2, 'sleeps': 3}\n",
        "\n",
        "        self.embeddings = np.array([\n",
        "            [0.2, -0.5, 0.1],   # the\n",
        "            [-0.3, 0.4, 0.2],   # cat\n",
        "            [0.1, 0.3, -0.4],   # sits\n",
        "            [-0.2, -0.1, 0.5]   # sleeps\n",
        "        ])\n",
        "\n",
        "        self.max_sequence_length = 10\n",
        "        self.position_dim = 3\n",
        "        self.position_embeddings = np.random.randn(self.max_sequence_length, self.position_dim) * 0.1\n",
        "\n",
        "        self.n_heads = 2\n",
        "        self.head_weights = np.random.randn(self.n_heads, 3, 3) * 0.1\n",
        "        self.head_importance = np.array([0.5, 0.5])\n",
        "\n",
        "        self.q_vectors = np.random.randn(self.n_heads, 3, 3) * 0.1\n",
        "        self.k_vectors = np.random.randn(self.n_heads, 3, 3) * 0.1\n",
        "        self.v_vectors = np.random.randn(self.n_heads, 3, 3) * 0.1\n",
        "\n",
        "    def attention(self, sentence):\n",
        "        print('TinyBert --- attention()')\n",
        "        print(sentence)\n",
        "        # convert to embeddings\n",
        "        word_ids = [self.word_to_id[word] for word in sentence]\n",
        "        print(word_ids)\n",
        "        vectors = self.embeddings[word_ids]\n",
        "        print(vectors)\n",
        "\n",
        "        positional_vectors = self.position_embeddings[:len(sentence)]\n",
        "        vectors = vectors + positional_vectors\n",
        "        print(vectors)\n",
        "\n",
        "        # calculate attention scores by seeing how aligned they are in terms of drections\n",
        "        # more similar words/embeddings pointing in same direction should have similar attentions\n",
        "        scores = np.dot(vectors, vectors.T)\n",
        "        print(scores)\n",
        "        scores = scores / np.sqrt(3) # scale scores\n",
        "        print(scores)\n",
        "\n",
        "        # convert to probabilities\n",
        "        probs = np.exp(scores)\n",
        "        print(probs)\n",
        "        attention_probs = probs / probs.sum(axis=1, keepdims=True)\n",
        "        print(attention_probs)\n",
        "\n",
        "        # work out new embeddings for words based on weighted sum of other words embeddings\n",
        "        newEmbeddings = np.dot(attention_probs, vectors)\n",
        "        print(newEmbeddings)\n",
        "        return attention_probs, newEmbeddings\n",
        "\n",
        "    def two_head_attention(self, sentence):\n",
        "        # Get word vectors like before\n",
        "        word_ids = [self.word_to_id[word] for word in sentence]\n",
        "        vectors = self.embeddings[word_ids] + self.position_embeddings[:len(sentence)]\n",
        "\n",
        "        # Create weights for two heads\n",
        "        head1_weights = np.random.randn(3, 3) * 0.1\n",
        "        head2_weights = np.random.randn(3, 3) * 0.1\n",
        "\n",
        "        # each head emphasizes a different aspect about the embedding - for example is an animal, is a verb, etc.\n",
        "        head1_vectors = np.dot(vectors, head1_weights)\n",
        "        head2_vectors = np.dot(vectors, head2_weights)\n",
        "\n",
        "        # the emphasized vectors then are multiplied itself for attention scores. if head1 emphasizes animals, then the animals in the vector will have similar attention\n",
        "        # the first step emphasizes attributes of embeddings for this step to detect them and allocate attention along similar words\n",
        "        head1_scores = np.dot(head1_vectors, head1_vectors.T)\n",
        "        head2_scores = np.dot(head2_vectors, head2_vectors.T)\n",
        "\n",
        "        # scale vectors\n",
        "        head1_vectors = head1_scores / np.sqrt(3)\n",
        "        head2_vectors = head2_scores / np.sqrt(3)\n",
        "\n",
        "        # convert to probabilities\n",
        "        head1_probs = np.exp(head1_vectors)\n",
        "        head2_probs = np.exp(head2_vectors)\n",
        "\n",
        "        # attention_probs\n",
        "        head1_probs = head1_probs / head1_probs.sum(axis=1, keepdims=True)\n",
        "        head2_probs = head2_probs / head2_probs.sum(axis=1, keepdims=True)\n",
        "\n",
        "        # new embeddings\n",
        "        head1_new_vectors = np.dot(head1_probs, vectors)\n",
        "        head2_new_vectors = np.dot(head2_probs, vectors)\n",
        "\n",
        "        # average between what attention embeddings say\n",
        "        newEmbeddings = (head1_new_vectors + head2_new_vectors)/2\n",
        "\n",
        "        return head1_probs, head2_probs, newEmbeddings\n",
        "\n",
        "    def n_head_attention(self, sentence):\n",
        "        # Get word vectors like before\n",
        "        word_ids = [self.word_to_id[word] for word in sentence]\n",
        "        vectors = self.embeddings[word_ids] + self.position_embeddings[:len(sentence)]\n",
        "\n",
        "        head_outputs = []\n",
        "        for i in range(self.n_heads):\n",
        "            # each head emphasizes a different aspect about the embedding - for example is an animal, is a verb, etc.\n",
        "            head_vectors = np.dot(vectors, self.head_weights[i])\n",
        "\n",
        "            # the emphasized vectors then are multiplied itself for attention scores. if head1 emphasizes animals, then the animals in the vector will have similar attention\n",
        "            # the first step emphasizes attributes of embeddings for this step to detect them and allocate attention along similar words\n",
        "            head_scores = np.dot(head_vectors, head_vectors.T)\n",
        "\n",
        "            # scale vectors\n",
        "            head_vectors = head_scores / np.sqrt(3)\n",
        "\n",
        "            # convert to probabilities\n",
        "            head_probs = np.exp(head_vectors)\n",
        "\n",
        "            # attention_probs\n",
        "            head_probs = head_probs / head_probs.sum(axis=1, keepdims=True)\n",
        "\n",
        "            # new embeddings\n",
        "            head_new_vectors = np.dot(head_probs, vectors)\n",
        "\n",
        "            head_outputs.append(head_new_vectors)\n",
        "\n",
        "        # stack head outputs\n",
        "        head_outputs = np.stack(head_outputs)\n",
        "\n",
        "        # weight head outputs\n",
        "        weighted_heads = head_outputs * self.head_importance[:, np.newaxis, np.newaxis]\n",
        "        # for each dimension, sum up head elements\n",
        "        newEmbeddings = weighted_heads.sum(axis=0)\n",
        "\n",
        "        return head_outputs, newEmbeddings\n",
        "\n",
        "    def QKV_attention(self, sentence):\n",
        "        # Get word vectors like before\n",
        "        word_ids = [self.word_to_id[word] for word in sentence]\n",
        "        vectors = self.embeddings[word_ids] + self.position_embeddings[:len(sentence)]\n",
        "\n",
        "        head_outputs = []\n",
        "        for i in range(self.n_heads):\n",
        "            # have to calculate qkv of vectors for head\n",
        "            q = np.dot(vectors, self.q_weights[i])  # What each word is looking for\n",
        "            k = np.dot(vectors, self.k_weights[i])  # What each word is advertising\n",
        "            v = np.dot(vectors, self.v_weights[i])  # What each word has to offer\n",
        "\n",
        "            # check out how similar query and key vectors are for this head\n",
        "            # this is attention score - words that have info for each other are related\n",
        "            queryKeySimilarity = np.dot(q, k.T)\n",
        "\n",
        "           # Scale scores to prevent exploding gradients\n",
        "            attention_scores = queryKeySimilarity / np.sqrt(3)\n",
        "\n",
        "            # Convert to probabilities with softmax\n",
        "            attention_probs = np.exp(attention_scores)\n",
        "            attention_probs = attention_probs / attention_probs.sum(axis=1, keepdims=True)\n",
        "\n",
        "            # new embedding by scaling value vectors by attention probabilities\n",
        "            head_output = np.dot(attention_probs, v)\n",
        "\n",
        "            head_outputs.append(head_output)\n",
        "\n",
        "        # stack head outputs\n",
        "        head_outputs = np.stack(head_outputs)\n",
        "\n",
        "        # weight head outputs\n",
        "        weighted_heads = head_outputs * self.head_importance[:, np.newaxis, np.newaxis]\n",
        "        # for each dimension, sum up head elements\n",
        "        newEmbeddings = weighted_heads.sum(axis=0)\n",
        "\n",
        "        return head_outputs, newEmbeddings\n",
        "\n",
        "\n",
        "\n",
        "    def calculate_head_loss(self, sentence, expected_relationships):\n",
        "        \"\"\"\n",
        "        expected_relationships: list of tuples (word1_idx, word2_idx, relationship_strength)\n",
        "        For example: [(0,1,1.0)] means word at index 0 should be strongly related to word at index 1\n",
        "        \"\"\"\n",
        "\n",
        "        # Get current attention patterns\n",
        "        head_attentions, final_embeddings = self.n_head_attention(sentence)\n",
        "\n",
        "        loss = 0\n",
        "        # For each expected relationship\n",
        "        for word1_idx, word2_idx, expected_strength in expected_relationships:\n",
        "            # Get similarity/attention between word embeddings\n",
        "            actual_strength = np.dot(final_embeddings[word1_idx], final_embeddings[word2_idx])\n",
        "            # Calculate difference from expected\n",
        "            loss += (expected_strength - actual_strength) ** 2\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def train_step(self, sentence, expected_relationships, learning_rate=0.01):\n",
        "        # Store original weights\n",
        "        original_weights = self.head_weights.copy()\n",
        "\n",
        "        # Calculate original loss\n",
        "        original_loss = self.calculate_head_loss(sentence, expected_relationships)\n",
        "\n",
        "        # Initialize gradients array\n",
        "        gradients = np.zeros_like(self.head_weights)\n",
        "\n",
        "        # For each head, for each word, for each dimension\n",
        "        for head in range(self.n_heads):\n",
        "            for i in range(3):  # dimension size\n",
        "                for j in range(3):\n",
        "                    # Slightly modify weight to compute gradient\n",
        "                    self.head_weights[head, i, j] += 0.0001\n",
        "                    new_loss = self.calculate_head_loss(sentence, expected_relationships)\n",
        "\n",
        "                    # Calculate and store gradient\n",
        "                    gradients[head, i, j] = (new_loss - original_loss) / 0.0001\n",
        "\n",
        "                    # Reset weight for next gradient calculation\n",
        "                    self.head_weights[head, i, j] = original_weights[head, i, j]\n",
        "\n",
        "        # Update all weights at once using calculated gradients\n",
        "        self.head_weights = original_weights - learning_rate * gradients"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert = TinyBERT()\n",
        "sentence = ['the', 'cat', 'sleeps']\n",
        "head1_result, head2_result = bert.animal_attention_example()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "BYDoKc4lEPJ-",
        "outputId": "e6f3c318-8c51-4723-8c41-e8e80fb14b5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Through animal-focused attention head:\n",
            "horse: [1.2 0.2 0.2]\n",
            "cat: [1. 0. 0.]\n",
            "bike: [0.1 0.1 0.1]\n",
            "\n",
            "Through ride-focused attention head:\n",
            "horse: [0.2 1.2 0.2]\n",
            "cat: [0.1 0.1 0.1]\n",
            "bike: [0. 1. 0.]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "cannot unpack non-iterable NoneType object",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e6ab770ddd13>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTinyBERT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'the'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sleeps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhead1_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead2_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manimal_attention_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert = TinyBERT()\n",
        "sentence = ['cat', 'sits', 'the', ]\n",
        "weights, new_reps = bert.attention(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VKSu1D54awu",
        "outputId": "8e91de6d-f58a-4a3f-9a53-b42a42dbade8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TinyBert --- attention()\n",
            "['cat', 'sits', 'the']\n",
            "[1, 2, 0]\n",
            "[[-0.3  0.4  0.2]\n",
            " [ 0.1  0.3 -0.4]\n",
            " [ 0.2 -0.5  0.1]]\n",
            "[[-0.25497954  0.4399019   0.11021631]\n",
            " [ 0.07694333  0.38871628 -0.50786617]\n",
            " [ 0.18630067 -0.41576185  0.31090431]]\n",
            "[[ 0.27067589  0.09540292 -0.19613056]\n",
            " [ 0.09540292  0.41494866 -0.30517659]\n",
            " [-0.19613056 -0.30517659  0.30422735]]\n",
            "[[ 0.1562748   0.0550809  -0.11323603]\n",
            " [ 0.0550809   0.23957072 -0.17619378]\n",
            " [-0.11323603 -0.17619378  0.17564574]]\n",
            "[[1.16914744 1.05662609 0.89293987]\n",
            " [1.05662609 1.27070355 0.83845549]\n",
            " [0.89293987 0.83845549 1.1920157 ]]\n",
            "[[0.37488133 0.33880192 0.28631675]\n",
            " [0.33376431 0.40138654 0.26484915]\n",
            " [0.30544451 0.28680725 0.40774824]]\n",
            "[[-0.01617752  0.17756925 -0.04173089]\n",
            " [-0.00487748  0.19273487 -0.08472163]\n",
            " [ 0.02014957  0.07632611  0.01477595]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nAttention weights:\")\n",
        "for i, word in enumerate(sentence):\n",
        "    print(f\"\\n{word} pays attention to each word:\")\n",
        "    for j, other_word in enumerate(sentence):\n",
        "        print(f\"{other_word}: {weights[i][j]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wtvrh8YC4_Uo",
        "outputId": "a790aed3-9cb9-4f1b-8545-759ac9f18177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Attention weights:\n",
            "\n",
            "cat pays attention to each word:\n",
            "cat: 0.375\n",
            "sits: 0.339\n",
            "the: 0.286\n",
            "\n",
            "sits pays attention to each word:\n",
            "cat: 0.334\n",
            "sits: 0.401\n",
            "the: 0.265\n",
            "\n",
            "the pays attention to each word:\n",
            "cat: 0.305\n",
            "sits: 0.287\n",
            "the: 0.408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from typing import Dict, List\n",
        "import torch.nn as nn\n",
        "\n",
        "class ModelInspector:\n",
        "    def __init__(self, model_name=\"BAAI/bge-large-en-v1.5\"):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name, output_attentions=True)\n",
        "        self.attention_outputs = {}\n",
        "\n",
        "    def inspect_text(self, text: str):\n",
        "        # Tokenize and get model outputs\n",
        "        inputs = self.tokenizer(text, return_tensors='pt', padding=True)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs, output_hidden_states=True, output_attentions=True)\n",
        "\n",
        "        print(\"\\n=== Token Embeddings ===\")\n",
        "        print(\"Shape:\", outputs.last_hidden_state.shape)\n",
        "        print(\"First token, first 5 values:\", outputs.last_hidden_state[0][0][:5].tolist())\n",
        "\n",
        "        print(\"\\n=== Attention Layers ===\")\n",
        "        # outputs.attentions is a tuple of attention tensors for each layer\n",
        "        for layer_idx, attention_layer in enumerate(outputs.attentions):\n",
        "            # attention_layer shape: [batch_size, num_heads, seq_length, seq_length]\n",
        "            print(f\"\\nLayer {layer_idx}:\")\n",
        "            print(f\"Shape: {attention_layer.shape}\")\n",
        "            print(f\"First head attention weights (first token):\")\n",
        "            print(attention_layer[0, 0, 0, :5].tolist())  # First 5 attention weights\n",
        "\n",
        "        print(\"\\n=== Final Pooled Output ===\")\n",
        "        print(\"Shape:\", outputs.pooler_output.shape)\n",
        "        print(\"First 5 values:\", outputs.pooler_output[0][:5].tolist())\n",
        "\n",
        "        return outputs\n",
        "\n",
        "# Example usage\n",
        "inspector = ModelInspector()\n",
        "text = \"This is a test blah.\"\n",
        "outputs = inspector.inspect_text(text)\n",
        "print(\"\\n=== Model Outputs ===\")\n",
        "print(outputs.pooler_output[0][:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byMf-evb4lbT",
        "outputId": "ace181ca-1974-4661-e55e-54b3103cd828"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Token Embeddings ===\n",
            "Shape: torch.Size([1, 8, 1024])\n",
            "First token, first 5 values: [0.47757381200790405, 0.19744722545146942, 0.29085487127304077, 0.5564297437667847, -1.2077648639678955]\n",
            "\n",
            "=== Attention Layers ===\n",
            "\n",
            "Layer 0:\n",
            "Shape: torch.Size([1, 16, 8, 8])\n",
            "First head attention weights (first token):\n",
            "[0.029770266264677048, 0.6047151684761047, 0.007282047998160124, 0.294673889875412, 0.00579837104305625]\n",
            "\n",
            "Layer 1:\n",
            "Shape: torch.Size([1, 16, 8, 8])\n",
            "First head attention weights (first token):\n",
            "[0.9059060215950012, 0.00976244080811739, 0.025496404618024826, 0.008608806878328323, 0.016318518668413162]\n",
            "\n",
            "Layer 2:\n",
            "Shape: torch.Size([1, 16, 8, 8])\n",
            "First head attention weights (first token):\n",
            "[0.8583857417106628, 0.012287343852221966, 0.011142122559249401, 0.01574944704771042, 0.010025042109191418]\n",
            "\n",
            "Layer 3:\n",
            "Shape: torch.Size([1, 16, 8, 8])\n",
            "First head attention weights (first token):\n",
            "[0.8044492602348328, 0.014801014214754105, 0.013178297318518162, 0.009986610151827335, 0.022480595856904984]\n",
            "\n",
            "Layer 4:\n",
            "Shape: torch.Size([1, 16, 8, 8])\n",
            "First head attention weights (first token):\n",
            "[0.5687976479530334, 0.017800936475396156, 0.017390862107276917, 0.009657816961407661, 0.03489385545253754]\n",
            "\n",
            "Layer 5:\n",
            "Shape: torch.Size([1, 16, 8, 8])\n",
            "First head attention weights (first token):\n",
            "[0.5738832950592041, 0.02638493850827217, 0.01991790533065796, 0.023784717544913292, 0.048180583864450455]\n",
            "\n",
            "Layer 6:\n",
            "Shape: torch.Size([1, 16, 8, 8])\n",
            "First head attention weights (first token):\n",
            "[0.5197768211364746, 0.018824541941285133, 0.02344648353755474, 0.016602426767349243, 0.029135925695300102]\n",
            "\n",
            "Layer 7:\n",
            "Shape: torch.Size([1, 16, 8, 8])\n",
            "First head attention weights (first token):\n",
            "[0.5252273082733154, 0.009654521010816097, 0.009589380584657192, 0.006926245987415314, 0.024576226249337196]\n",
            "\n",
            "Layer 8:\n",
            "Shape: torch.Size([1, 16, 8, 8])\n",
            "First head attention weights (first token):\n",
            "[0.34685078263282776, 0.013334370218217373, 0.012735678814351559, 0.009536033496260643, 0.011987928301095963]\n",
            "\n",
            "Layer 9:\n",
            "Shape: torch.Size([1, 16, 8, 8])\n",
            "First head attention weights (first token):\n",
            "[0.15983113646507263, 0.07789270579814911, 0.05983928218483925, 0.07406838983297348, 0.052795037627220154]\n",
            "\n",
            "Layer 10:\n",
            "Shape: torch.Size([1, 16, 8, 8])\n",
            "First head attention weights (first token):\n",
            "[0.12981677055358887, 0.0721585676074028, 0.09387370198965073, 0.0863291546702385, 0.04594750329852104]\n",
            "\n",
            "Layer 11:\n",
            "Shape: torch.Size([1, 16, 8, 8])\n",
            "First head attention weights (first token):\n",
            "[0.17308923602104187, 0.023812217637896538, 0.01669022999703884, 0.02798282727599144, 0.03817388787865639]\n",
            "\n",
            "Layer 12:\n",
            "Shape: torch.Size([1, 16, 8, 8])\n",
            "First head attention weights (first token):\n",
            "[0.07747059315443039, 0.07616209983825684, 0.06568825989961624, 0.04511602967977524, 0.06864229589700699]\n",
            "\n",
            "Layer 13:\n",
            "Shape: torch.Size([1, 16, 8, 8])\n",
            "First head attention weights (first token):\n",
            "[0.014231620356440544, 0.010939577594399452, 0.002845283132046461, 0.0033995613921433687, 0.01204405166208744]\n",
            "\n",
            "Layer 14:\n",
            "Shape: torch.Size([1, 16, 8, 8])\n",
            "First head attention weights (first token):\n",
            "[0.14102064073085785, 0.01827275939285755, 0.004753577057272196, 0.001811488764360547, 0.006799023598432541]\n",
            "\n",
            "Layer 15:\n",
            "Shape: torch.Size([1, 16, 8, 8])\n",
            "First head attention weights (first token):\n",
            "[0.13347797095775604, 0.09730146825313568, 0.07912349700927734, 0.03591695427894592, 0.026321403682231903]\n",
            "\n",
            "Layer 16:\n",
            "Shape: torch.Size([1, 16, 8, 8])\n",
            "First head attention weights (first token):\n",
            "[0.13147400319576263, 0.008520758710801601, 0.005951032042503357, 0.0023155396338552237, 0.012553254142403603]\n",
            "\n",
            "Layer 17:\n",
            "Shape: torch.Size([1, 16, 8, 8])\n",
            "First head attention weights (first token):\n",
            "[0.09391475468873978, 0.047028765082359314, 0.013401887379586697, 0.008425748907029629, 0.005584010388702154]\n",
            "\n",
            "Layer 18:\n",
            "Shape: torch.Size([1, 16, 8, 8])\n",
            "First head attention weights (first token):\n",
            "[0.14214427769184113, 0.027403900399804115, 0.006207215134054422, 0.006560135167092085, 0.005828820634633303]\n",
            "\n",
            "Layer 19:\n",
            "Shape: torch.Size([1, 16, 8, 8])\n",
            "First head attention weights (first token):\n",
            "[0.13642561435699463, 0.06327199935913086, 0.007262061350047588, 0.005826982669532299, 0.03775424137711525]\n",
            "\n",
            "Layer 20:\n",
            "Shape: torch.Size([1, 16, 8, 8])\n",
            "First head attention weights (first token):\n",
            "[0.01725299470126629, 0.02178727649152279, 0.012053068727254868, 0.010558318346738815, 0.024340344592928886]\n",
            "\n",
            "Layer 21:\n",
            "Shape: torch.Size([1, 16, 8, 8])\n",
            "First head attention weights (first token):\n",
            "[0.07080922275781631, 0.04552074894309044, 0.03594542294740677, 0.016926642507314682, 0.003551129950210452]\n",
            "\n",
            "Layer 22:\n",
            "Shape: torch.Size([1, 16, 8, 8])\n",
            "First head attention weights (first token):\n",
            "[0.019110923632979393, 0.062230221927165985, 0.02683405764400959, 0.05386103317141533, 0.263704776763916]\n",
            "\n",
            "Layer 23:\n",
            "Shape: torch.Size([1, 16, 8, 8])\n",
            "First head attention weights (first token):\n",
            "[0.06730493158102036, 0.052509188652038574, 0.06950061023235321, 0.04389676824212074, 0.08776077628135681]\n",
            "\n",
            "=== Final Pooled Output ===\n",
            "Shape: torch.Size([1, 1024])\n",
            "First 5 values: [-0.8635739088058472, -0.5718868374824524, -0.5205396413803101, -0.9303846955299377, -0.5613108277320862]\n",
            "\n",
            "=== Model Outputs ===\n",
            "tensor([-0.8636, -0.5719, -0.5205, -0.9304, -0.5613])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from typing import Dict, List, Tuple\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "class EnhancedModelInspector:\n",
        "    def __init__(self, model_name=\"BAAI/bge-large-en-v1.5\"):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name, output_attentions=True)\n",
        "        self.layer_stats = defaultdict(dict)\n",
        "\n",
        "    def get_parameter_stats(self) -> Dict:\n",
        "        \"\"\"Analyze model parameters and their statistics.\"\"\"\n",
        "        stats = {}\n",
        "        total_params = 0\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                stats[name] = {\n",
        "                    'shape': tuple(param.shape),\n",
        "                    'mean': param.mean().item(),\n",
        "                    'std': param.std().item(),\n",
        "                    'min': param.min().item(),\n",
        "                    'max': param.max().item(),\n",
        "                    'num_params': param.numel(),\n",
        "                    'sparsity': (param == 0).float().mean().item(),\n",
        "                    'gradient_enabled': param.requires_grad\n",
        "                }\n",
        "                total_params += param.numel()\n",
        "        stats['total_parameters'] = total_params\n",
        "        return stats\n",
        "\n",
        "    def analyze_attention_patterns(self, attention_weights: torch.Tensor) -> Dict:\n",
        "        \"\"\"Analyze attention patterns in detail.\"\"\"\n",
        "        # attention_weights shape: [batch_size, num_heads, seq_length, seq_length]\n",
        "        attention_analysis = {\n",
        "            'shape': attention_weights.shape,\n",
        "            'entropy': self._compute_attention_entropy(attention_weights),\n",
        "            'head_importance': self._compute_head_importance(attention_weights),\n",
        "            'attention_sparsity': (attention_weights == 0).float().mean().item(),\n",
        "            'max_attention': attention_weights.max().item(),\n",
        "            'mean_attention': attention_weights.mean().item()\n",
        "        }\n",
        "        return attention_analysis\n",
        "\n",
        "    def _compute_attention_entropy(self, attention_weights: torch.Tensor) -> float:\n",
        "        \"\"\"Compute attention entropy as a measure of focus/dispersion.\"\"\"\n",
        "        # Avoid log(0) by adding small epsilon\n",
        "        eps = 1e-10\n",
        "        entropy = -(attention_weights * torch.log(attention_weights + eps)).sum(dim=-1).mean().item()\n",
        "        return entropy\n",
        "\n",
        "    def _compute_head_importance(self, attention_weights: torch.Tensor) -> List[float]:\n",
        "        \"\"\"Compute importance scores for each attention head based on attention magnitude.\"\"\"\n",
        "        return attention_weights.mean(dim=[0, 2, 3]).tolist()\n",
        "\n",
        "    def analyze_hidden_states(self, hidden_states: Tuple[torch.Tensor]) -> Dict:\n",
        "        \"\"\"Analyze hidden state activations across layers.\"\"\"\n",
        "        hidden_analysis = {}\n",
        "        for layer_idx, hidden_state in enumerate(hidden_states):\n",
        "            hidden_analysis[f'layer_{layer_idx}'] = {\n",
        "                'shape': hidden_state.shape,\n",
        "                'mean_activation': hidden_state.mean().item(),\n",
        "                'std_activation': hidden_state.std().item(),\n",
        "                'activation_sparsity': (hidden_state == 0).float().mean().item(),\n",
        "                'max_activation': hidden_state.max().item(),\n",
        "                'min_activation': hidden_state.min().item()\n",
        "            }\n",
        "        return hidden_analysis\n",
        "\n",
        "    def inspect_text(self, text: str, verbose: bool = True) -> Dict:\n",
        "        \"\"\"Perform comprehensive model inspection for given text.\"\"\"\n",
        "        # Tokenize and get model outputs\n",
        "        inputs = self.tokenizer(text, return_tensors='pt', padding=True)\n",
        "        tokens = self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs, output_hidden_states=True, output_attentions=True)\n",
        "\n",
        "        # Comprehensive analysis\n",
        "        analysis = {\n",
        "            'text_length': len(text),\n",
        "            'num_tokens': len(tokens),\n",
        "            'tokens': tokens,\n",
        "            'token_ids': inputs['input_ids'][0].tolist(),\n",
        "            'parameter_stats': self.get_parameter_stats(),\n",
        "            'attention_analysis': {},\n",
        "            'hidden_state_analysis': self.analyze_hidden_states(outputs.hidden_states),\n",
        "            'pooled_output_stats': {\n",
        "                'shape': outputs.pooler_output.shape,\n",
        "                'mean': outputs.pooler_output.mean().item(),\n",
        "                'std': outputs.pooler_output.std().item(),\n",
        "                'first_values': outputs.pooler_output[0][:10].tolist()\n",
        "            }\n",
        "        }\n",
        "        # Extract weights\n",
        "        embedding_weights = self.model.embeddings.word_embeddings.weight\n",
        "        layer_0_qkv = {\n",
        "            'query': self.model.encoder.layer[0].attention.self.query.weight,\n",
        "            'key': self.model.encoder.layer[0].attention.self.key.weight,\n",
        "            'value': self.model.encoder.layer[0].attention.self.value.weight\n",
        "        }\n",
        "        print(layer_0_qkv)\n",
        "\n",
        "        # Analyze attention patterns for each layer\n",
        "        for layer_idx, attention_layer in enumerate(outputs.attentions):\n",
        "            analysis['attention_analysis'][f'layer_{layer_idx}'] = self.analyze_attention_patterns(attention_layer)\n",
        "\n",
        "        if verbose:\n",
        "            self._print_analysis(analysis)\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    def _print_analysis(self, analysis: Dict):\n",
        "        \"\"\"Pretty print the analysis results.\"\"\"\n",
        "        print(\"\\n=== Model Analysis Report ===\")\n",
        "\n",
        "        print(\"\\n1. Input Text Statistics:\")\n",
        "        print(f\"Text length: {analysis['text_length']} characters\")\n",
        "        print(f\"Number of tokens: {analysis['num_tokens']}\")\n",
        "        print(\"First 10 tokens:\", analysis['tokens'][:10])\n",
        "\n",
        "        print(\"\\n2. Model Parameters Overview:\")\n",
        "        print(f\"Total parameters: {analysis['parameter_stats']['total_parameters']:,}\")\n",
        "\n",
        "        print(\"\\n3. Attention Analysis:\")\n",
        "        for layer_name, layer_data in analysis['attention_analysis'].items():\n",
        "            print(f\"\\n{layer_name}:\")\n",
        "            print(f\"Attention entropy: {layer_data['entropy']:.4f}\")\n",
        "            print(f\"Attention sparsity: {layer_data['attention_sparsity']:.4f}\")\n",
        "            print(f\"Mean attention: {layer_data['mean_attention']:.4f}\")\n",
        "\n",
        "        print(\"\\n4. Hidden State Analysis:\")\n",
        "        for layer_name, layer_data in analysis['hidden_state_analysis'].items():\n",
        "            print(f\"\\n{layer_name}:\")\n",
        "            print(f\"Mean activation: {layer_data['mean_activation']:.4f}\")\n",
        "            print(f\"Activation sparsity: {layer_data['activation_sparsity']:.4f}\")\n",
        "\n",
        "        print(\"\\n5. Pooled Output Statistics:\")\n",
        "        print(f\"Shape: {analysis['pooled_output_stats']['shape']}\")\n",
        "        print(f\"Mean: {analysis['pooled_output_stats']['mean']:.4f}\")\n",
        "        print(f\"Standard deviation: {analysis['pooled_output_stats']['std']:.4f}\")\n",
        "\n",
        "# Example usage\n",
        "inspector = EnhancedModelInspector()\n",
        "text = \"This is a test sentence for analysis.\"\n",
        "analysis = inspector.inspect_text(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGPRYUcei5Q_",
        "outputId": "2d519eef-3655-40a6-d060-378f2993d5a2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': Parameter containing:\n",
            "tensor([[ 0.0196, -0.0512, -0.0378,  ...,  0.0012, -0.0132,  0.0288],\n",
            "        [ 0.0232,  0.0321,  0.0128,  ..., -0.0620,  0.0114, -0.0528],\n",
            "        [ 0.0332, -0.0264, -0.0607,  ...,  0.0434,  0.0133, -0.0662],\n",
            "        ...,\n",
            "        [-0.0061,  0.0046, -0.0341,  ...,  0.0408,  0.0260,  0.0653],\n",
            "        [ 0.0475,  0.0067,  0.0667,  ..., -0.0076,  0.0105, -0.0408],\n",
            "        [ 0.0256, -0.0007, -0.0361,  ...,  0.0175,  0.0473,  0.0349]],\n",
            "       requires_grad=True), 'key': Parameter containing:\n",
            "tensor([[-0.0544,  0.0370, -0.0173,  ...,  0.0019,  0.0319,  0.0513],\n",
            "        [-0.0426,  0.0233, -0.0215,  ..., -0.0511,  0.0392,  0.0296],\n",
            "        [ 0.0237,  0.0242,  0.0622,  ...,  0.0844,  0.0631, -0.0113],\n",
            "        ...,\n",
            "        [-0.0196,  0.0176, -0.0171,  ..., -0.0310,  0.0187,  0.0322],\n",
            "        [-0.0029,  0.0076,  0.0066,  ...,  0.0197, -0.0158,  0.0461],\n",
            "        [ 0.0136, -0.0209, -0.0289,  ..., -0.0079,  0.0190, -0.0126]],\n",
            "       requires_grad=True), 'value': Parameter containing:\n",
            "tensor([[-0.0103, -0.0053,  0.0349,  ...,  0.0050, -0.0217,  0.0019],\n",
            "        [-0.0247, -0.0132, -0.0230,  ...,  0.0278,  0.0309,  0.0263],\n",
            "        [-0.0057, -0.0034, -0.0638,  ..., -0.0414,  0.0248,  0.0021],\n",
            "        ...,\n",
            "        [ 0.0273, -0.0272,  0.0061,  ...,  0.0144,  0.0187, -0.0112],\n",
            "        [-0.0396, -0.0167, -0.0067,  ..., -0.0124, -0.0113,  0.0269],\n",
            "        [ 0.0040, -0.0225,  0.0023,  ...,  0.0078,  0.0100, -0.0074]],\n",
            "       requires_grad=True)}\n",
            "\n",
            "=== Model Analysis Report ===\n",
            "\n",
            "1. Input Text Statistics:\n",
            "Text length: 37 characters\n",
            "Number of tokens: 10\n",
            "First 10 tokens: ['[CLS]', 'this', 'is', 'a', 'test', 'sentence', 'for', 'analysis', '.', '[SEP]']\n",
            "\n",
            "2. Model Parameters Overview:\n",
            "Total parameters: 335,141,888\n",
            "\n",
            "3. Attention Analysis:\n",
            "\n",
            "layer_0:\n",
            "Attention entropy: 1.5128\n",
            "Attention sparsity: 0.0000\n",
            "Mean attention: 0.1000\n",
            "\n",
            "layer_1:\n",
            "Attention entropy: 1.0078\n",
            "Attention sparsity: 0.0000\n",
            "Mean attention: 0.1000\n",
            "\n",
            "layer_2:\n",
            "Attention entropy: 1.1287\n",
            "Attention sparsity: 0.0000\n",
            "Mean attention: 0.1000\n",
            "\n",
            "layer_3:\n",
            "Attention entropy: 1.2510\n",
            "Attention sparsity: 0.0000\n",
            "Mean attention: 0.1000\n",
            "\n",
            "layer_4:\n",
            "Attention entropy: 1.4889\n",
            "Attention sparsity: 0.0000\n",
            "Mean attention: 0.1000\n",
            "\n",
            "layer_5:\n",
            "Attention entropy: 1.3896\n",
            "Attention sparsity: 0.0000\n",
            "Mean attention: 0.1000\n",
            "\n",
            "layer_6:\n",
            "Attention entropy: 1.3270\n",
            "Attention sparsity: 0.0000\n",
            "Mean attention: 0.1000\n",
            "\n",
            "layer_7:\n",
            "Attention entropy: 1.3113\n",
            "Attention sparsity: 0.0000\n",
            "Mean attention: 0.1000\n",
            "\n",
            "layer_8:\n",
            "Attention entropy: 1.4738\n",
            "Attention sparsity: 0.0000\n",
            "Mean attention: 0.1000\n",
            "\n",
            "layer_9:\n",
            "Attention entropy: 1.3123\n",
            "Attention sparsity: 0.0000\n",
            "Mean attention: 0.1000\n",
            "\n",
            "layer_10:\n",
            "Attention entropy: 1.3130\n",
            "Attention sparsity: 0.0000\n",
            "Mean attention: 0.1000\n",
            "\n",
            "layer_11:\n",
            "Attention entropy: 0.9189\n",
            "Attention sparsity: 0.0000\n",
            "Mean attention: 0.1000\n",
            "\n",
            "layer_12:\n",
            "Attention entropy: 1.0141\n",
            "Attention sparsity: 0.0000\n",
            "Mean attention: 0.1000\n",
            "\n",
            "layer_13:\n",
            "Attention entropy: 0.9049\n",
            "Attention sparsity: 0.0000\n",
            "Mean attention: 0.1000\n",
            "\n",
            "layer_14:\n",
            "Attention entropy: 0.8893\n",
            "Attention sparsity: 0.0000\n",
            "Mean attention: 0.1000\n",
            "\n",
            "layer_15:\n",
            "Attention entropy: 1.0674\n",
            "Attention sparsity: 0.0000\n",
            "Mean attention: 0.1000\n",
            "\n",
            "layer_16:\n",
            "Attention entropy: 1.1367\n",
            "Attention sparsity: 0.0000\n",
            "Mean attention: 0.1000\n",
            "\n",
            "layer_17:\n",
            "Attention entropy: 1.4346\n",
            "Attention sparsity: 0.0000\n",
            "Mean attention: 0.1000\n",
            "\n",
            "layer_18:\n",
            "Attention entropy: 1.4705\n",
            "Attention sparsity: 0.0000\n",
            "Mean attention: 0.1000\n",
            "\n",
            "layer_19:\n",
            "Attention entropy: 1.2992\n",
            "Attention sparsity: 0.0000\n",
            "Mean attention: 0.1000\n",
            "\n",
            "layer_20:\n",
            "Attention entropy: 1.1207\n",
            "Attention sparsity: 0.0000\n",
            "Mean attention: 0.1000\n",
            "\n",
            "layer_21:\n",
            "Attention entropy: 1.1647\n",
            "Attention sparsity: 0.0000\n",
            "Mean attention: 0.1000\n",
            "\n",
            "layer_22:\n",
            "Attention entropy: 1.3694\n",
            "Attention sparsity: 0.0000\n",
            "Mean attention: 0.1000\n",
            "\n",
            "layer_23:\n",
            "Attention entropy: 1.9070\n",
            "Attention sparsity: 0.0000\n",
            "Mean attention: 0.1000\n",
            "\n",
            "4. Hidden State Analysis:\n",
            "\n",
            "layer_0:\n",
            "Mean activation: 0.0258\n",
            "Activation sparsity: 0.0000\n",
            "\n",
            "layer_1:\n",
            "Mean activation: 0.0221\n",
            "Activation sparsity: 0.0000\n",
            "\n",
            "layer_2:\n",
            "Mean activation: 0.0171\n",
            "Activation sparsity: 0.0000\n",
            "\n",
            "layer_3:\n",
            "Mean activation: 0.0156\n",
            "Activation sparsity: 0.0000\n",
            "\n",
            "layer_4:\n",
            "Mean activation: 0.0119\n",
            "Activation sparsity: 0.0000\n",
            "\n",
            "layer_5:\n",
            "Mean activation: 0.0142\n",
            "Activation sparsity: 0.0000\n",
            "\n",
            "layer_6:\n",
            "Mean activation: 0.0110\n",
            "Activation sparsity: 0.0000\n",
            "\n",
            "layer_7:\n",
            "Mean activation: 0.0008\n",
            "Activation sparsity: 0.0000\n",
            "\n",
            "layer_8:\n",
            "Mean activation: -0.0051\n",
            "Activation sparsity: 0.0000\n",
            "\n",
            "layer_9:\n",
            "Mean activation: -0.0117\n",
            "Activation sparsity: 0.0000\n",
            "\n",
            "layer_10:\n",
            "Mean activation: -0.0085\n",
            "Activation sparsity: 0.0000\n",
            "\n",
            "layer_11:\n",
            "Mean activation: 0.0061\n",
            "Activation sparsity: 0.0000\n",
            "\n",
            "layer_12:\n",
            "Mean activation: 0.0078\n",
            "Activation sparsity: 0.0000\n",
            "\n",
            "layer_13:\n",
            "Mean activation: 0.0041\n",
            "Activation sparsity: 0.0000\n",
            "\n",
            "layer_14:\n",
            "Mean activation: 0.0088\n",
            "Activation sparsity: 0.0000\n",
            "\n",
            "layer_15:\n",
            "Mean activation: 0.0051\n",
            "Activation sparsity: 0.0000\n",
            "\n",
            "layer_16:\n",
            "Mean activation: -0.0079\n",
            "Activation sparsity: 0.0000\n",
            "\n",
            "layer_17:\n",
            "Mean activation: -0.0108\n",
            "Activation sparsity: 0.0000\n",
            "\n",
            "layer_18:\n",
            "Mean activation: -0.0143\n",
            "Activation sparsity: 0.0000\n",
            "\n",
            "layer_19:\n",
            "Mean activation: -0.0057\n",
            "Activation sparsity: 0.0000\n",
            "\n",
            "layer_20:\n",
            "Mean activation: 0.0007\n",
            "Activation sparsity: 0.0000\n",
            "\n",
            "layer_21:\n",
            "Mean activation: -0.0123\n",
            "Activation sparsity: 0.0000\n",
            "\n",
            "layer_22:\n",
            "Mean activation: -0.0110\n",
            "Activation sparsity: 0.0000\n",
            "\n",
            "layer_23:\n",
            "Mean activation: -0.0047\n",
            "Activation sparsity: 0.0000\n",
            "\n",
            "layer_24:\n",
            "Mean activation: -0.0032\n",
            "Activation sparsity: 0.0000\n",
            "\n",
            "5. Pooled Output Statistics:\n",
            "Shape: torch.Size([1, 1024])\n",
            "Mean: 0.0226\n",
            "Standard deviation: 0.7696\n"
          ]
        }
      ]
    }
  ]
}